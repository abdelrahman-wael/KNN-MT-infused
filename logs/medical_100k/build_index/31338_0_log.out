submitit INFO (2022-10-10 02:00:49,644) - Starting with JobEnvironment(job_id=31338, hostname=az-eus-v100-32gb-6-worker-00093, local_rank=0(1), node=0(1), global_rank=0(1))
submitit INFO (2022-10-10 02:00:49,645) - Loading pickle: /home/ababouelenin/CustomTranslation/test_gen/knnmt/logs/medical_100k/build_index/31338_submitted.pkl
The following command is sent: "python fairseq_cli/generate.py /home/ababouelenin/CustomTranslation/knnmt/blob_data/data/medical/medical/preprocessed --gen-subset train --path /home/ababouelenin/CustomTranslation/knnmt/blob_data/training_dir/checkpoints_deen_pretrained/checkpoint_best.pt --beam 5 --remove-bpe --bpe sentencepiece --num-workers 35 --max-sentences 2 -s de -t en --source-lang de --target-lang en --tokenizer moses --moses-source-lang de --moses-target-lang en --sacrebleu --score-reference --dstore-mmap dstores/medical_100k/index_only.subset.0 --knn-keytype last_ffn_input --model-overrides {'knn_keytype':'last_ffn_input'} --save-knn-dstore --save-knn-subset --save-knn-subset-num 248099"
2022-10-10 02:00:52 | INFO | fairseq_cli.generate | Namespace(all_gather_list_size=16384, beam=5, bf16=False, bpe='sentencepiece', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', cpu=False, criterion='cross_entropy', data='/home/ababouelenin/CustomTranslation/knnmt/blob_data/data/medical/medical/preprocessed', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoding_format=None, device_id=0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, drop_lang_tok=False, dstore_filename=None, dstore_fp16=False, dstore_mmap='dstores/medical_100k/index_only.subset.0', dstore_size=9651607, empty_cache_freq=0, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, faiss_metric_type='l2', fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='train', indexfile=None, iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, k=1024, knn_add_idx_global_id=None, knn_add_idx_pos_in_dataset=None, knn_add_num_to_idx=10000000, knn_add_to_idx=False, knn_backoff=False, knn_backoff_thresh=None, knn_embed_dim=None, knn_keytype='last_ffn_input', knn_proc=-1, knn_q2gpu=False, knn_sim_func=None, knn_start=-1, knn_temp=1.0, knn_trim_data=False, knnmt=False, left_pad_source='True', left_pad_target='False', lenpen=1, lmbda=0.0, load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=2, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_bf16=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides="{'knn_keytype':'last_ffn_input'}", model_parallel_size=1, momentum=0.99, moses_no_dash_splits=False, moses_no_escape=False, moses_source_lang='de', moses_target_lang='en', move_dstore_to_mem=False, nbest=1, no_beamable_mm=False, no_early_stop=False, no_load_keys=False, no_progress_bar=False, no_repeat_ngram_size=0, no_seed_provided=True, nprocs_per_node=8, num_batch_buckets=0, num_shards=1, num_workers=35, optimizer='nag', path='/home/ababouelenin/CustomTranslation/knnmt/blob_data/training_dir/checkpoints_deen_pretrained/checkpoint_best.pt', prefix_size=0, print_alignment=False, print_step=False, probe=8, profile=False, quantization_config_path=None, quiet=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, retain_dropout=False, retain_dropout_modules=None, retain_iter_history=False, sacrebleu=True, sampling=False, sampling_topk=-1, sampling_topp=-1.0, save_knn_dstore=True, save_knn_subset=True, save_knn_subset_num=248099, save_knns=False, save_knns_filename=None, score_reference=True, scoring='bleu', seed=1, sentencepiece_model='/home/ababouelenin/CustomTranslation/knnmt/blob_data/vocab/deen_spm/sentencepiece.bpe.model', shard_id=0, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', target_lang='en', task='translation', temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer='moses', tpu=False, trained_index=None, truncate_source=False, unkpen=0, unnormalized=False, upsample_primary=1, use_faiss_only=False, user_dir=None, warmup_updates=0, weight_decay=0.0, write_index=None)
2022-10-10 02:00:52 | INFO | fairseq.tasks.translation | [de] dictionary: 32001 types
2022-10-10 02:00:52 | INFO | fairseq.tasks.translation | [en] dictionary: 32001 types
2022-10-10 02:00:52 | INFO | fairseq.data.data_utils | loaded 248099 examples from: /home/ababouelenin/CustomTranslation/knnmt/blob_data/data/medical/medical/preprocessed/train.de-en.de
2022-10-10 02:00:52 | INFO | fairseq.data.data_utils | loaded 248099 examples from: /home/ababouelenin/CustomTranslation/knnmt/blob_data/data/medical/medical/preprocessed/train.de-en.en
2022-10-10 02:00:52 | INFO | fairseq.tasks.translation | /home/ababouelenin/CustomTranslation/knnmt/blob_data/data/medical/medical/preprocessed train de-en 248099 examples
2022-10-10 02:00:52 | INFO | fairseq_cli.generate | loading model(s) from /home/ababouelenin/CustomTranslation/knnmt/blob_data/training_dir/checkpoints_deen_pretrained/checkpoint_best.pt
keytype being saved: last_ffn_input
Saving fp32
2022-10-10 02:01:03 | INFO | fairseq.logging.progress_bar | :    101 / 124050 wps=1590
2022-10-10 02:01:04 | INFO | fairseq.logging.progress_bar | :    201 / 124050 wps=2260
2022-10-10 02:01:06 | INFO | fairseq.logging.progress_bar | :    301 / 124050 wps=2494
2022-10-10 02:01:07 | INFO | fairseq.logging.progress_bar | :    401 / 124050 wps=2688
2022-10-10 02:01:09 | INFO | fairseq.logging.progress_bar | :    501 / 124050 wps=2797
2022-10-10 02:01:10 | INFO | fairseq.logging.progress_bar | :    601 / 124050 wps=2893
2022-10-10 02:01:12 | INFO | fairseq.logging.progress_bar | :    701 / 124050 wps=2994
2022-10-10 02:01:13 | INFO | fairseq.logging.progress_bar | :    801 / 124050 wps=3060
2022-10-10 02:01:15 | INFO | fairseq.logging.progress_bar | :    901 / 124050 wps=3079
2022-10-10 02:01:16 | INFO | fairseq.logging.progress_bar | :   1001 / 124050 wps=3119
2022-10-10 02:01:18 | INFO | fairseq.logging.progress_bar | :   1101 / 124050 wps=3156
2022-10-10 02:01:19 | INFO | fairseq.logging.progress_bar | :   1201 / 124050 wps=3164
2022-10-10 02:01:21 | INFO | fairseq.logging.progress_bar | :   1301 / 124050 wps=3211
2022-10-10 02:01:22 | INFO | fairseq.logging.progress_bar | :   1401 / 124050 wps=3222
2022-10-10 02:01:24 | INFO | fairseq.logging.progress_bar | :   1501 / 124050 wps=3224
2022-10-10 02:01:26 | INFO | fairseq.logging.progress_bar | :   1601 / 124050 wps=3223
2022-10-10 02:01:27 | INFO | fairseq.logging.progress_bar | :   1701 / 124050 wps=3234
2022-10-10 02:01:29 | INFO | fairseq.logging.progress_bar | :   1801 / 124050 wps=3250
2022-10-10 02:01:30 | INFO | fairseq.logging.progress_bar | :   1901 / 124050 wps=3248
2022-10-10 02:01:32 | INFO | fairseq.logging.progress_bar | :   2001 / 124050 wps=3267
2022-10-10 02:01:33 | INFO | fairseq.logging.progress_bar | :   2101 / 124050 wps=3259
2022-10-10 02:01:35 | INFO | fairseq.logging.progress_bar | :   2201 / 124050 wps=3278
2022-10-10 02:01:36 | INFO | fairseq.logging.progress_bar | :   2301 / 124050 wps=3282
2022-10-10 02:01:38 | INFO | fairseq.logging.progress_bar | :   2401 / 124050 wps=3288
2022-10-10 02:01:39 | INFO | fairseq.logging.progress_bar | :   2501 / 124050 wps=3297
2022-10-10 02:01:41 | INFO | fairseq.logging.progress_bar | :   2601 / 124050 wps=3304
2022-10-10 02:01:42 | INFO | fairseq.logging.progress_bar | :   2701 / 124050 wps=3306
2022-10-10 02:01:44 | INFO | fairseq.logging.progress_bar | :   2801 / 124050 wps=3315
2022-10-10 02:01:45 | INFO | fairseq.logging.progress_bar | :   2901 / 124050 wps=3332
2022-10-10 02:01:47 | INFO | fairseq.logging.progress_bar | :   3001 / 124050 wps=3333
2022-10-10 02:01:48 | INFO | fairseq.logging.progress_bar | :   3101 / 124050 wps=3347
2022-10-10 02:01:50 | INFO | fairseq.logging.progress_bar | :   3201 / 124050 wps=3339
2022-10-10 02:01:51 | INFO | fairseq.logging.progress_bar | :   3301 / 124050 wps=3342
2022-10-10 02:01:53 | INFO | fairseq.logging.progress_bar | :   3401 / 124050 wps=3347
2022-10-10 02:01:55 | INFO | fairseq.logging.progress_bar | :   3501 / 124050 wps=3352
2022-10-10 02:01:56 | INFO | fairseq.logging.progress_bar | :   3601 / 124050 wps=3362
2022-10-10 02:01:58 | INFO | fairseq.logging.progress_bar | :   3701 / 124050 wps=3363
2022-10-10 02:01:59 | INFO | fairseq.logging.progress_bar | :   3801 / 124050 wps=3362
2022-10-10 02:02:01 | INFO | fairseq.logging.progress_bar | :   3901 / 124050 wps=3364
2022-10-10 02:02:02 | INFO | fairseq.logging.progress_bar | :   4001 / 124050 wps=3368
2022-10-10 02:02:04 | INFO | fairseq.logging.progress_bar | :   4101 / 124050 wps=3373
2022-10-10 02:02:05 | INFO | fairseq.logging.progress_bar | :   4201 / 124050 wps=3376
2022-10-10 02:02:07 | INFO | fairseq.logging.progress_bar | :   4301 / 124050 wps=3373
2022-10-10 02:02:08 | INFO | fairseq.logging.progress_bar | :   4401 / 124050 wps=3375
2022-10-10 02:02:10 | INFO | fairseq.logging.progress_bar | :   4501 / 124050 wps=3388
2022-10-10 02:02:11 | INFO | fairseq.logging.progress_bar | :   4601 / 124050 wps=3396
dstore_idx 248099 final number of items added 3 total saved 248099
Keys (248099, 512) float32
Vals (248099, 1) int64
2022-10-10 02:02:12 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2
2022-10-10 02:02:12 | INFO | fairseq_cli.generate | Translated 9364 sentences (248125 tokens) in 64.7s (144.69 sentences/s, 3833.84 tokens/s)
submitit INFO (2022-10-10 02:02:14,619) - Job completed successfully
